"""
ç¤ºä¾‹ 3: å¤šè½®å¯¹è¯ä¸ä¸Šä¸‹æ–‡å‹ç¼©

æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ ConversationHistory è¿›è¡Œå¤šè½®å¯¹è¯ï¼Œ
ä»¥åŠä¸Šä¸‹æ–‡è¿‡é•¿æ—¶çš„è‡ªåŠ¨å‹ç¼©åŠŸèƒ½ã€‚
"""

import asyncio
import os
from ame.foundation.llm import OpenAICaller


async def demo_basic_multi_turn():
    """æ¼”ç¤ºåŸºç¡€å¤šè½®å¯¹è¯"""
    print("\n" + "=" * 60)
    print("åŸºç¡€å¤šè½®å¯¹è¯æ¼”ç¤º")
    print("=" * 60)
    
    # åˆå§‹åŒ– LLM
    llm = OpenAICaller(
        api_key=os.getenv("OPENAI_API_KEY", "sk-..."),
        model="gpt-3.5-turbo",
        max_context_tokens=1000  # è®¾ç½®è¾ƒå°çš„ä¸Šä¸‹æ–‡é™åˆ¶ç”¨äºæ¼”ç¤º
    )
    
    # åˆ›å»ºå¯¹è¯å†å²
    conversation = llm.create_conversation(
        system_prompt="ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„ AI åŠ©æ‰‹ï¼Œæ“…é•¿å›ç­”ç¼–ç¨‹ç›¸å…³é—®é¢˜ã€‚"
    )
    
    print(f"\nâœ… å¯¹è¯å·²åˆ›å»º (æœ€å¤§ä¸Šä¸‹æ–‡: {llm.max_context_tokens} tokens)")
    
    # å¤šè½®å¯¹è¯
    questions = [
        "ä»€ä¹ˆæ˜¯ Python çš„å¼‚æ­¥ç¼–ç¨‹ï¼Ÿ",
        "èƒ½ç»™æˆ‘ä¸€ä¸ªå…·ä½“çš„ä¾‹å­å—ï¼Ÿ",
        "åœ¨å®é™…é¡¹ç›®ä¸­å¦‚ä½•ä½¿ç”¨å®ƒï¼Ÿ",
        "æœ‰ä»€ä¹ˆå¸¸è§çš„å‘å—ï¼Ÿ"
    ]
    
    for i, question in enumerate(questions, 1):
        print(f"\n--- ç¬¬ {i} è½®å¯¹è¯ ---")
        print(f"ğŸ‘¤ ç”¨æˆ·: {question}")
        
        # ä½¿ç”¨å¯¹è¯å†å²è¿›è¡Œå¯¹è¯
        response = await llm.chat_with_history(
            conversation=conversation,
            user_message=question,
            temperature=0.7
        )
        
        print(f"ğŸ¤– AI: {response.content[:200]}...")
        print(f"ğŸ“Š Token ä½¿ç”¨: {response.usage['total_tokens']}")
        print(f"ğŸ’¬ å†å²æ¶ˆæ¯æ•°: {conversation.get_message_count()}")


async def demo_streaming_multi_turn():
    """æ¼”ç¤ºæµå¼å¤šè½®å¯¹è¯"""
    print("\n" + "=" * 60)
    print("æµå¼å¤šè½®å¯¹è¯æ¼”ç¤º")
    print("=" * 60)
    
    llm = OpenAICaller(
        api_key=os.getenv("OPENAI_API_KEY", "sk-..."),
        model="gpt-3.5-turbo"
    )
    
    conversation = llm.create_conversation(
        system_prompt="ä½ æ˜¯ä¸€ä¸ªè®²æ•…äº‹é«˜æ‰‹ï¼Œå–„äºç»­å†™æ•…äº‹ã€‚"
    )
    
    print(f"\nâœ… å¯¹è¯å·²åˆ›å»º")
    
    # æµå¼å¯¹è¯
    prompts = [
        "ç»™æˆ‘è®²ä¸€ä¸ªå…³äºå‹‡æ°”çš„æ•…äº‹çš„å¼€å¤´",
        "ç„¶åå‘¢ï¼Ÿç»§ç»­è®²",
        "æœ€åæ€ä¹ˆæ ·äº†ï¼Ÿ"
    ]
    
    for i, prompt in enumerate(prompts, 1):
        print(f"\n--- ç¬¬ {i} è½®å¯¹è¯ ---")
        print(f"ğŸ‘¤ ç”¨æˆ·: {prompt}")
        print("ğŸ¤– AI: ", end="", flush=True)
        
        async for chunk in llm.chat_stream_with_history(
            conversation=conversation,
            user_message=prompt,
            temperature=0.8
        ):
            print(chunk, end="", flush=True)
        
        print("\n")


async def demo_context_compression():
    """æ¼”ç¤ºä¸Šä¸‹æ–‡è‡ªåŠ¨å‹ç¼©"""
    print("\n" + "=" * 60)
    print("ä¸Šä¸‹æ–‡è‡ªåŠ¨å‹ç¼©æ¼”ç¤º")
    print("=" * 60)
    
    # è®¾ç½®è¾ƒå°çš„ä¸Šä¸‹æ–‡é™åˆ¶ï¼Œä¾¿äºè§¦å‘å‹ç¼©
    llm = OpenAICaller(
        api_key=os.getenv("OPENAI_API_KEY", "sk-..."),
        model="gpt-3.5-turbo",
        max_context_tokens=500  # æ•…æ„è®¾ç½®å¾—å¾ˆå°
    )
    
    conversation = llm.create_conversation(
        system_prompt="ä½ æ˜¯ä¸€ä¸ª AI åŠ©æ‰‹ã€‚"
    )
    
    print(f"\nâœ… å¯¹è¯å·²åˆ›å»º (æœ€å¤§ä¸Šä¸‹æ–‡: {llm.max_context_tokens} tokens)")
    print("âš ï¸  ç”±äºä¸Šä¸‹æ–‡é™åˆ¶è¾ƒå°ï¼Œå°†æ¼”ç¤ºè‡ªåŠ¨å‹ç¼©åŠŸèƒ½")
    
    # è¿›è¡Œå¤šè½®å¯¹è¯ï¼Œé€æ¸å¡«æ»¡ä¸Šä¸‹æ–‡
    messages = [
        "è¯·è¯¦ç»†ä»‹ç»ä¸€ä¸‹ Python çš„å†å²å’Œå‘å±•",
        "Python æœ‰å“ªäº›ä¸»è¦çš„åº”ç”¨é¢†åŸŸï¼Ÿ",
        "Python çš„ä¼˜ç¼ºç‚¹åˆ†åˆ«æ˜¯ä»€ä¹ˆï¼Ÿ",
        "Python 2 å’Œ Python 3 æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ",
        "å¦‚ä½•å­¦ä¹  Pythonï¼Ÿ",
        "Python çš„æœªæ¥å‘å±•è¶‹åŠ¿æ˜¯ä»€ä¹ˆï¼Ÿ"
    ]
    
    for i, msg in enumerate(messages, 1):
        print(f"\n--- ç¬¬ {i} è½®å¯¹è¯ ---")
        print(f"ğŸ‘¤ ç”¨æˆ·: {msg}")
        
        # è®¡ç®—å½“å‰ä¸Šä¸‹æ–‡ token æ•°
        current_messages = conversation.get_messages()
        total_tokens = sum(
            llm.estimate_tokens(m.get("content", "")) 
            for m in current_messages
        )
        
        print(f"ğŸ“Š å½“å‰ä¸Šä¸‹æ–‡ tokens: {total_tokens}/{llm.max_context_tokens}")
        
        # å‘é€æ¶ˆæ¯
        response = await llm.chat_with_history(
            conversation=conversation,
            user_message=msg,
            temperature=0.7
        )
        
        print(f"ğŸ¤– AI: {response.content[:150]}...")
        print(f"ğŸ’¬ å†å²æ¶ˆæ¯æ•°: {conversation.get_message_count()}")
        
        # æ£€æŸ¥æ˜¯å¦è§¦å‘äº†å‹ç¼©
        if "compressed" in response.metadata:
            print("ğŸ”„ è§¦å‘äº†ä¸Šä¸‹æ–‡å‹ç¼©ï¼")


async def demo_manual_compression():
    """æ¼”ç¤ºæ‰‹åŠ¨å‹ç¼©åŠŸèƒ½"""
    print("\n" + "=" * 60)
    print("æ‰‹åŠ¨å‹ç¼©æ¼”ç¤º")
    print("=" * 60)
    
    llm = OpenAICaller(
        api_key=os.getenv("OPENAI_API_KEY", "sk-..."),
        model="gpt-3.5-turbo"
    )
    
    # åˆ›å»ºä¸€ä¸ªè¾ƒé•¿çš„æ¶ˆæ¯åˆ—è¡¨
    messages = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ª AI åŠ©æ‰‹ã€‚"},
        {"role": "user", "content": "ç¬¬ä¸€ä¸ªé—®é¢˜ï¼š" + "a" * 100},
        {"role": "assistant", "content": "ç¬¬ä¸€ä¸ªå›ç­”ï¼š" + "b" * 100},
        {"role": "user", "content": "ç¬¬äºŒä¸ªé—®é¢˜ï¼š" + "c" * 100},
        {"role": "assistant", "content": "ç¬¬äºŒä¸ªå›ç­”ï¼š" + "d" * 100},
        {"role": "user", "content": "ç¬¬ä¸‰ä¸ªé—®é¢˜ï¼š" + "e" * 100},
        {"role": "assistant", "content": "ç¬¬ä¸‰ä¸ªå›ç­”ï¼š" + "f" * 100},
    ]
    
    print(f"\nåŸå§‹æ¶ˆæ¯æ•°: {len(messages)}")
    
    # è®¡ç®—æ€» token æ•°
    total_tokens = sum(llm.estimate_tokens(msg["content"]) for msg in messages)
    print(f"åŸå§‹ token æ•°: {total_tokens}")
    
    # å‹ç¼©åˆ° 200 tokens
    compressed = llm.compress_messages(messages, max_tokens=200)
    
    print(f"\nå‹ç¼©åæ¶ˆæ¯æ•°: {len(compressed)}")
    compressed_tokens = sum(llm.estimate_tokens(msg["content"]) for msg in compressed)
    print(f"å‹ç¼©å token æ•°: {compressed_tokens}")
    
    print(f"\nâœ… å‹ç¼©æ•ˆæœ:")
    print(f"  - ç§»é™¤æ¶ˆæ¯æ•°: {len(messages) - len(compressed)}")
    print(f"  - èŠ‚çœ tokens: {total_tokens - compressed_tokens}")
    print(f"  - å‹ç¼©ç‡: {(1 - compressed_tokens/total_tokens)*100:.1f}%")
    
    # æ˜¾ç¤ºå‹ç¼©åçš„æ¶ˆæ¯
    print(f"\nğŸ“‹ å‹ç¼©åçš„æ¶ˆæ¯:")
    for msg in compressed:
        role = msg["role"]
        content = msg["content"][:50] + "..." if len(msg["content"]) > 50 else msg["content"]
        print(f"  [{role}] {content}")


async def demo_token_estimation():
    """æ¼”ç¤º token ä¼°ç®—åŠŸèƒ½"""
    print("\n" + "=" * 60)
    print("Token ä¼°ç®—æ¼”ç¤º")
    print("=" * 60)
    
    llm = OpenAICaller(
        api_key=os.getenv("OPENAI_API_KEY", "sk-..."),
        model="gpt-3.5-turbo"
    )
    
    test_texts = [
        "Hello, world!",
        "ä½ å¥½ï¼Œä¸–ç•Œï¼",
        "This is a longer English sentence with more words.",
        "è¿™æ˜¯ä¸€ä¸ªæ›´é•¿çš„ä¸­æ–‡å¥å­ï¼ŒåŒ…å«æ›´å¤šçš„å­—ç¬¦ã€‚",
        "æ··åˆ Mixed æ–‡æœ¬ text with ä¸­è‹±æ–‡ English and Chinese.",
    ]
    
    print("\nğŸ“Š Token ä¼°ç®—ç»“æœ:")
    print("-" * 60)
    
    for text in test_texts:
        tokens = llm.estimate_tokens(text)
        chars = len(text)
        ratio = chars / tokens if tokens > 0 else 0
        
        print(f"\næ–‡æœ¬: {text}")
        print(f"  - å­—ç¬¦æ•°: {chars}")
        print(f"  - ä¼°ç®— tokens: {tokens}")
        print(f"  - å­—ç¬¦/token æ¯”: {ratio:.2f}")


async def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("å¤šè½®å¯¹è¯ä¸ä¸Šä¸‹æ–‡å‹ç¼©ç¤ºä¾‹")
    print("=" * 60)
    
    # è¿è¡Œå„ä¸ªæ¼”ç¤º
    await demo_basic_multi_turn()
    await demo_streaming_multi_turn()
    await demo_context_compression()
    await demo_manual_compression()
    await demo_token_estimation()
    
    # æ€»ç»“
    print("\n" + "=" * 60)
    print("âœ¨ æ¼”ç¤ºå®Œæˆï¼")
    print("=" * 60)
    print("\nğŸ“– å…³é”®åŠŸèƒ½:")
    print("  1. ConversationHistory - å¤šè½®å¯¹è¯å†å²ç®¡ç†")
    print("  2. chat_with_history() - ä½¿ç”¨å†å²çš„å¯¹è¯")
    print("  3. chat_stream_with_history() - æµå¼å¤šè½®å¯¹è¯")
    print("  4. è‡ªåŠ¨ä¸Šä¸‹æ–‡å‹ç¼© - è¶…è¿‡é™åˆ¶æ—¶æ— æ„Ÿå‹ç¼©")
    print("  5. estimate_tokens() - ç²¾ç¡®çš„ token ä¼°ç®—")
    print("  6. compress_messages() - æ‰‹åŠ¨å‹ç¼©æ¶ˆæ¯")
    print("\nğŸ’¡ å‹ç¼©ç­–ç•¥:")
    print("  - ä¿ç•™ç³»ç»Ÿæ¶ˆæ¯ï¼ˆsystemï¼‰")
    print("  - ä¿ç•™æœ€æ–°çš„å¯¹è¯ï¼ˆä»æ–°åˆ°æ—§ï¼‰")
    print("  - è‡ªåŠ¨ç§»é™¤æœ€æ—©çš„æ¶ˆæ¯")
    print("  - æ—¥å¿—è®°å½•å‹ç¼©è¿‡ç¨‹")


if __name__ == "__main__":
    asyncio.run(main())
ÃŸ